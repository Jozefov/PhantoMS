{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.096973Z",
     "start_time": "2025-02-02T16:07:50.217665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from massspecgym.data.datasets import MSnDataset\n",
    "from massspecgym.featurize import SpectrumFeaturizer\n",
    "from massspecgym.data import RetrievalDataset, MassSpecDataModule\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import typing as T\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "from massspecgym.models.base import Stage\n",
    "from massspecgym.models.de_novo.base import DeNovoMassSpecGymModel\n",
    "\n",
    "from phantoms.utils.custom_tokenizers import ByteBPETokenizerWithSpecialTokens\n",
    "from phantoms.utils.constants import PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN\n",
    "\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer"
   ],
   "id": "64795c335ce9b8cf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.213935Z",
     "start_time": "2025-02-02T16:07:53.187032Z"
    }
   },
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional encoding as in \"Attention is All You Need\".\n",
    "    Expects shape [seq_len, batch_size, d_model] if batch_first=False.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # shape becomes [max_len, d_model]\n",
    "\n",
    "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer(\"pe\", pe)  # not a learnable parameter\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        Returns the input plus positional encodings.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(0)\n",
    "        # Add the encoding only up to seq_len\n",
    "        # self.pe[:seq_len] is [seq_len, 1, d_model]\n",
    "        return x + self.pe[:seq_len]\n",
    "\n",
    "class FreezeDecoderCallback(pl.Callback):\n",
    "    def __init__(self, freeze_epochs: int = 3):\n",
    "        self.freeze_epochs = freeze_epochs\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch < self.freeze_epochs:\n",
    "            for param in pl_module.decoder_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in pl_module.transformer_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in pl_module.decoder_fc.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in pl_module.pos_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"Epoch {trainer.current_epoch}: Pretrained decoder frozen.\")\n",
    "        else:\n",
    "            for param in pl_module.decoder_embed.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in pl_module.transformer_decoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in pl_module.decoder_fc.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in pl_module.pos_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(f\"Epoch {trainer.current_epoch}: Pretrained decoder unfrozen.\")\n",
    "            \n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list, tokenizer, max_len=200):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.smiles_list[idx]\n",
    "        # Encode the text with special tokens (the post-processor adds SOS and EOS)\n",
    "        token_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        # Truncate if necessary\n",
    "        token_ids = token_ids[:self.max_len]\n",
    "        # For teacher forcing, input is all tokens except the last,\n",
    "        # target is all tokens except the first.\n",
    "        input_ids = token_ids[:-1]\n",
    "        target_ids = token_ids[1:]\n",
    "        return {\n",
    "            \"input\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class SMILESLanguageModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 4,\n",
    "                 num_decoder_layers: int = 4, dropout: float = 0.1,\n",
    "                 pad_token_id: int = 0, max_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
    "            dropout=dropout, activation=\"relu\", batch_first=False\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "    def forward(self, tgt_input):\n",
    "        # tgt_input: [seq_len, batch]\n",
    "        emb = self.embedding(tgt_input) * math.sqrt(self.d_model)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        batch = tgt_input.size(1)\n",
    "        # For standalone LM, we can use a zero \"memory\"\n",
    "        memory = torch.zeros(1, batch, self.d_model, device=tgt_input.device)\n",
    "        seq_len = tgt_input.size(0)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=tgt_input.device), diagonal=1)\n",
    "        output = self.decoder(tgt=emb, memory=memory, tgt_mask=causal_mask)\n",
    "        logits = self.fc_out(output)\n",
    "        return logits\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(batch[\"input\"])  # [seq_len, batch, vocab_size]\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), batch[\"target\"].view(-1))\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "class GATDeNovoTransformer(DeNovoMassSpecGymModel):\n",
    "    \"\"\"\n",
    "    Transformer model for de novo SMILES generation with a GAT encoder and a multi-layer Transformer decoder.\n",
    "    \n",
    "    When collect_embeddings=True in the forward pass, the returned dictionary (under key \"embeddings\") contains:\n",
    "      - \"gnn_i\": global mean pooled output after GAT layer i.\n",
    "      - \"gnn_i_head_j\": per-head pooled outputs from GAT layer i (for each head j).\n",
    "      - \"encoder_projection\": output of encoder_fc after (optionally) concatenating the formula branch.\n",
    "      - For each decoder layer i:\n",
    "            • \"decoder_layer_i\": mean-pooled output of that decoder layer.\n",
    "            • \"decoder_layer_i_head_j\": projection of the mean per head j of that layer (each projected to d_model).\n",
    "    \n",
    "    Decoding supports greedy (beam_width=1) and beam search (beam_width>1).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,              # e.g., 4000\n",
    "        d_model: int = 1024,         # desired model dimension (e.g., 1024)\n",
    "        nhead: int = 8,\n",
    "        num_gat_layers: int = 3,\n",
    "        num_decoder_layers: int = 6,\n",
    "        num_gat_heads: int = 8,\n",
    "        gat_dropout: float = 0.6,\n",
    "        smiles_tokenizer: ByteBPETokenizerWithSpecialTokens = None,\n",
    "        start_token: str = SOS_TOKEN,\n",
    "        end_token: str = EOS_TOKEN,\n",
    "        pad_token: str = PAD_TOKEN,\n",
    "        unk_token: str = UNK_TOKEN,\n",
    "        dropout: float = 0.1,\n",
    "        max_smiles_len: int = 200,\n",
    "        k_predictions: int = 1,\n",
    "        temperature: T.Optional[float] = 1.0,\n",
    "        pre_norm: bool = False,\n",
    "        chemical_formula: bool = False,\n",
    "        formula_embedding_dim: int = 64,  # used if chemical_formula is True\n",
    "        log_only_loss_at_stages: Optional[list] = [Stage.TRAIN],\n",
    "        test_beam_width: int = 1,  # default greedy during training; can be set higher at test\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(log_only_loss_at_stages=log_only_loss_at_stages, *args, **kwargs)\n",
    "        if smiles_tokenizer is None:\n",
    "            raise ValueError(\"Must provide a ByteBPETokenizerWithSpecialTokens instance.\")\n",
    "        self.smiles_tokenizer = smiles_tokenizer\n",
    "        self.vocab_size = self.smiles_tokenizer.get_vocab_size()\n",
    "        for tok in [start_token, end_token, pad_token, unk_token]:\n",
    "            if tok not in self.smiles_tokenizer.get_vocab():\n",
    "                raise ValueError(f\"Special token '{tok}' not in tokenizer vocab\")\n",
    "        self.start_token_id = self.smiles_tokenizer.token_to_id(start_token)\n",
    "        self.end_token_id   = self.smiles_tokenizer.token_to_id(end_token)\n",
    "        self.pad_token_id   = self.smiles_tokenizer.token_to_id(pad_token)\n",
    "        self.unk_token_id   = self.smiles_tokenizer.token_to_id(unk_token)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_smiles_len = max_smiles_len\n",
    "        self.k_predictions = k_predictions\n",
    "        self.temperature = temperature if k_predictions > 1 else None\n",
    "        self.chemical_formula = chemical_formula\n",
    "        self.test_beam_width = test_beam_width\n",
    "        self.nhead = nhead  # number of heads\n",
    "        \n",
    "        # --- GAT Encoder ---\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        # Use a GATConv with concatenation; its output shape will be [num_nodes, nhead * (d_model/nhead)] = [num_nodes, d_model]\n",
    "        self.gat_layers.append(\n",
    "            GATConv(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=d_model // num_gat_heads,\n",
    "                heads=num_gat_heads,\n",
    "                dropout=gat_dropout,\n",
    "                add_self_loops=True\n",
    "            )\n",
    "        )\n",
    "        for _ in range(num_gat_layers - 1):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(\n",
    "                    in_channels=d_model,\n",
    "                    out_channels=d_model // num_gat_heads,\n",
    "                    heads=num_gat_heads,\n",
    "                    dropout=gat_dropout,\n",
    "                    add_self_loops=True\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # --- Encoder Projection & Formula Integration ---\n",
    "        if self.chemical_formula:\n",
    "            # Formula encoder: maps from 128 to formula_embedding_dim\n",
    "            self.formula_encoder = nn.Sequential(\n",
    "                nn.Linear(128, formula_embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(formula_embedding_dim, formula_embedding_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            # After concatenation, input dimension is (d_model + formula_embedding_dim)\n",
    "            self.encoder_fc = nn.Linear(d_model + formula_embedding_dim, d_model)\n",
    "        else:\n",
    "            self.encoder_fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # --- Transformer Decoder ---\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"relu\",\n",
    "            batch_first=False,\n",
    "            norm_first=pre_norm\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_smiles_len)\n",
    "        self.decoder_embed = nn.Embedding(self.vocab_size, d_model, padding_idx=self.pad_token_id)\n",
    "        self.decoder_fc = nn.Linear(d_model, self.vocab_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)\n",
    "        \n",
    "        # --- Per-head projections for decoder layers ---\n",
    "        # One projection per decoder layer to map each head's vector (d_model//nhead) to d_model.\n",
    "        self.decoder_head_projs = nn.ModuleList([\n",
    "            nn.Linear(d_model // nhead, d_model) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.encoder_fc.weight)\n",
    "        nn.init.zeros_(self.encoder_fc.bias)\n",
    "        nn.init.normal_(self.decoder_embed.weight, mean=0, std=self.d_model**-0.5)\n",
    "        nn.init.xavier_uniform_(self.decoder_fc.weight)\n",
    "        nn.init.zeros_(self.decoder_fc.bias)\n",
    "        \n",
    "    def step(self, batch: dict, stage: Stage = Stage.NONE) -> dict:\n",
    "        ret = self.forward(batch, collect_embeddings=False)\n",
    "        loss = ret[\"loss\"]\n",
    "        self.log(f\"{stage.to_pref()}loss\", loss, prog_bar=True, batch_size=batch[\"spec\"].num_graphs)\n",
    "        if stage not in self.log_only_loss_at_stages:\n",
    "            mols_pred = self.decode_smiles(batch, beam_width=self.test_beam_width)\n",
    "            ret[\"mols_pred\"] = mols_pred\n",
    "        else:\n",
    "            ret[\"mols_pred\"] = None\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch: dict, collect_embeddings: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass with teacher forcing.\n",
    "        If collect_embeddings is True, a dictionary with intermediate embeddings is returned.\n",
    "        \"\"\"\n",
    "        embeddings: Dict[str, torch.Tensor] = {}\n",
    "        spec = batch[\"spec\"]\n",
    "        smiles_list = batch[\"mol\"]\n",
    "        x, edge_index, batch_idx = spec.x, spec.edge_index, spec.batch\n",
    "        \n",
    "        # --- GAT Encoder with per-head extraction ---\n",
    "        for i, gat in enumerate(self.gat_layers, 1):\n",
    "            x = gat(x, edge_index)  # shape: [num_nodes, d_model] (concatenated output)\n",
    "            x = F.elu(x)\n",
    "            # Overall pooled output (always needed)\n",
    "            # pooled = global_mean_pool(x, batch_idx)  # [batch, d_model]\n",
    "            if collect_embeddings:\n",
    "                pooled = global_mean_pool(x, batch_idx)  # [batch, d_model]\n",
    "                embeddings[f\"gnn_{i}\"] = pooled.detach()\n",
    "                # Also extract per-head embeddings\n",
    "                head_dim = self.d_model // self.nhead  # compute only if needed\n",
    "                x_heads = x.view(x.size(0), self.nhead, head_dim)  # reshape to [num_nodes, nhead, head_dim]\n",
    "                for h in range(self.nhead):\n",
    "                    head_h = x_heads[:, h, :]  # [num_nodes, head_dim]\n",
    "                    pooled_head = global_mean_pool(head_h, batch_idx)  # [batch, head_dim]\n",
    "                    # Store each head’s pooled embedding\n",
    "                    embeddings[f\"gnn_{i}_head_{h+1}\"] = pooled_head.detach()\n",
    "        gnn_out = global_mean_pool(x, batch_idx)  # final overall pooled output\n",
    "        # if collect_embeddings:\n",
    "        #     embeddings[\"gnn_pool\"] = gnn_out.detach()\n",
    "            \n",
    "        # --- Formula Integration ---\n",
    "        if self.chemical_formula and (\"formula\" in batch):\n",
    "            formula = batch[\"formula\"].float().to(x.device)\n",
    "            formula_enc = self.formula_encoder(formula)  # [batch, formula_embedding_dim]\n",
    "            combined = torch.cat([gnn_out, formula_enc], dim=1)  # [batch, d_model + formula_embedding_dim]\n",
    "        else:\n",
    "            combined = gnn_out\n",
    "        # if collect_embeddings:\n",
    "        #     embeddings[\"encoder_input\"] = combined.detach()\n",
    "        encoder_proj = self.encoder_fc(combined)  # [batch, d_model]\n",
    "        if collect_embeddings:\n",
    "            embeddings[\"encoder_projection\"] = encoder_proj.detach()\n",
    "        memory = encoder_proj.unsqueeze(0)  # [1, batch, d_model]\n",
    "        \n",
    "        # --- Transformer Decoder Teacher Forcing ---\n",
    "        encoded_smiles = self.smiles_tokenizer.encode_batch(smiles_list)  # list of lists of token IDs\n",
    "        smiles_ids = torch.tensor(encoded_smiles, dtype=torch.long, device=x.device)\n",
    "        tgt_input  = smiles_ids[:, :-1]\n",
    "        tgt_output = smiles_ids[:, 1:]\n",
    "        tgt_input  = tgt_input.transpose(0, 1).contiguous()   # [seq_len-1, batch]\n",
    "        tgt_output = tgt_output.transpose(0, 1).contiguous()     # [seq_len-1, batch]\n",
    "        tgt_key_padding_mask = (tgt_input == self.pad_token_id).transpose(0, 1)\n",
    "        tgt_embed = self.decoder_embed(tgt_input) * math.sqrt(self.d_model)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        tgt_len = tgt_input.size(0)\n",
    "        causal_mask = torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=x.device), diagonal=1)\n",
    "        \n",
    "        # --- Transformer Decoder: iterate layer-by-layer ---\n",
    "        decoder_input = tgt_embed\n",
    "        if collect_embeddings:\n",
    "            for i, layer in enumerate(self.transformer_decoder.layers, 1):\n",
    "                decoder_input = layer(decoder_input, memory, tgt_mask=causal_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "                pooled_dec = decoder_input.mean(dim=0)  # [batch, d_model]\n",
    "                embeddings[f\"decoder_layer_{i}\"] = pooled_dec.detach()\n",
    "                d_head = self.d_model // self.nhead\n",
    "                reshaped = decoder_input.view(decoder_input.size(0), decoder_input.size(1), self.nhead, d_head)\n",
    "                head_means = reshaped.mean(dim=0)  # [batch, nhead, d_head]\n",
    "                for h in range(self.nhead):\n",
    "                    head_emb = self.decoder_head_projs[i-1](head_means[:, h, :])  # [batch, d_model]\n",
    "                    embeddings[f\"decoder_layer_{i}_head_{h+1}\"] = head_emb.detach()\n",
    "        else:\n",
    "            # If not collecting embeddings, simply run the whole decoder:\n",
    "            decoder_input = self.transformer_decoder(\n",
    "                tgt=tgt_embed, memory=memory, tgt_mask=causal_mask, tgt_key_padding_mask=tgt_key_padding_mask\n",
    "            )\n",
    "        \n",
    "        logits = self.decoder_fc(decoder_input)\n",
    "        logits = logits.transpose(0, 1).contiguous()  # [batch, seq_len-1, vocab_size]\n",
    "        tgt_output = tgt_output.transpose(0, 1).contiguous()  # [batch, seq_len-1]\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), tgt_output.view(-1))\n",
    "        ret = {\"loss\": loss}\n",
    "        if collect_embeddings:\n",
    "            ret[\"embeddings\"] = embeddings\n",
    "        return ret\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    def beam_search_decode(self, memory: torch.Tensor, beam_width: int, max_length: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Performs beam search decoding for a single sample.\n",
    "        memory: [d_model] tensor.\n",
    "        Returns the best sequence (list of token IDs).\n",
    "        \"\"\"\n",
    "        device = memory.device\n",
    "        memory = memory.unsqueeze(0).unsqueeze(0)  # [1, 1, d_model]\n",
    "        beam = [([self.start_token_id], 0.0, False)]\n",
    "        for _ in range(max_length):\n",
    "            new_beam = []\n",
    "            for seq, log_prob, finished in beam:\n",
    "                if finished:\n",
    "                    new_beam.append((seq, log_prob, finished))\n",
    "                    continue\n",
    "                seq_tensor = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(1)  # [seq_len, 1]\n",
    "                tgt_embed = self.decoder_embed(seq_tensor) * math.sqrt(self.d_model)\n",
    "                tgt_embed = self.pos_encoder(tgt_embed)\n",
    "                seq_len = seq_tensor.size(0)\n",
    "                causal_mask = self._generate_square_subsequent_mask(seq_len).to(device)\n",
    "                output = self.transformer_decoder(tgt=tgt_embed, memory=memory, tgt_mask=causal_mask, memory_key_padding_mask=None)\n",
    "                logits = self.decoder_fc(output[-1])  # [1, vocab_size]\n",
    "                log_probs = F.log_softmax(logits, dim=-1).squeeze(0)  # [vocab_size]\n",
    "                topk = torch.topk(log_probs, beam_width)\n",
    "                for token, token_log_prob in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "                    new_seq = seq + [token]\n",
    "                    new_log_prob = log_prob + token_log_prob\n",
    "                    new_finished = (token == self.end_token_id)\n",
    "                    new_beam.append((new_seq, new_log_prob, new_finished))\n",
    "            new_beam.sort(key=lambda x: x[1], reverse=True)\n",
    "            beam = new_beam[:beam_width]\n",
    "            if all(candidate[2] for candidate in beam):\n",
    "                break\n",
    "        best_seq, _, _ = max(beam, key=lambda x: x[1])\n",
    "        return best_seq\n",
    "\n",
    "    def decode_smiles(self, batch: dict, beam_width: Optional[int] = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Generate SMILES for each sample in the batch.\n",
    "        If beam_width is not provided, uses self.test_beam_width.\n",
    "        Returns a list (length = batch_size) of lists (each containing one generated SMILES).\n",
    "        \"\"\"\n",
    "        if beam_width is None:\n",
    "            beam_width = self.test_beam_width\n",
    "        spec = batch[\"spec\"]\n",
    "        x, edge_index, batch_idx = spec.x, spec.edge_index, spec.batch\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch_idx)\n",
    "        if self.chemical_formula and (\"formula\" in batch):\n",
    "            formula = batch[\"formula\"].float().to(x.device)\n",
    "            formula_enc = self.formula_encoder(formula)\n",
    "            x = torch.cat([x, formula_enc], dim=1)\n",
    "        memory_all = self.encoder_fc(x)\n",
    "        batch_size = memory_all.size(0)\n",
    "        device = memory_all.device\n",
    "        decoded_list = []\n",
    "        if beam_width <= 1:\n",
    "            generated_tokens = torch.full((1, batch_size), self.start_token_id, dtype=torch.long, device=device)\n",
    "            finished = [False] * batch_size\n",
    "            decoded_sequences = [[] for _ in range(batch_size)]\n",
    "            for _ in range(self.max_smiles_len):\n",
    "                tgt_embed = self.decoder_embed(generated_tokens) * math.sqrt(self.d_model)\n",
    "                tgt_embed = self.pos_encoder(tgt_embed)\n",
    "                tgt_len = tgt_embed.size(0)\n",
    "                causal_mask = self._generate_square_subsequent_mask(tgt_len).to(device)\n",
    "                output = self.transformer_decoder(tgt=tgt_embed, memory=memory_all.unsqueeze(0), tgt_mask=causal_mask)\n",
    "                last_logits = self.decoder_fc(output[-1])\n",
    "                next_token = torch.argmax(last_logits, dim=-1).unsqueeze(0)\n",
    "                generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "                for i in range(batch_size):\n",
    "                    if not finished[i]:\n",
    "                        token_id = next_token[0, i].item()\n",
    "                        if token_id == self.end_token_id:\n",
    "                            finished[i] = True\n",
    "                        else:\n",
    "                            decoded_sequences[i].append(token_id)\n",
    "                if all(finished):\n",
    "                    break\n",
    "            for seq in decoded_sequences:\n",
    "                text = self.smiles_tokenizer.decode(seq, skip_special_tokens=True)\n",
    "                decoded_list.append([text])\n",
    "        else:\n",
    "            for i in range(batch_size):\n",
    "                mem = memory_all[i]\n",
    "                best_seq = self.beam_search_decode(mem, beam_width, self.max_smiles_len)\n",
    "                text = self.smiles_tokenizer.decode(best_seq, skip_special_tokens=True)\n",
    "                decoded_list.append([text])\n",
    "        return decoded_list\n",
    "\n",
    "    def get_embeddings(self, batch: dict, smiles_batch: Optional[List[str]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run a forward pass with collect_embeddings=True and return a dictionary of intermediate embeddings.\n",
    "        The returned dictionary contains keys:\n",
    "          - \"gnn_i\" for each GAT layer overall pooled output,\n",
    "          - \"gnn_i_head_j\" for per-head pooled outputs of each GAT layer,\n",
    "          - \"encoder_projection\" for the output of encoder_fc,\n",
    "          - \"decoder_layer_i\" for overall pooled output of each decoder layer,\n",
    "          - \"decoder_layer_i_head_j\" for per-head pooled outputs from each decoder layer.\n",
    "        \"\"\"\n",
    "        ret = self.forward(batch, collect_embeddings=True)\n",
    "        return ret.get(\"embeddings\", {})"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.218134Z",
     "start_time": "2025-02-02T16:07:53.216706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spectra_mgf = \"/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/min_sample_trees.mgf\"\n",
    "split_file = \"/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/20241211_split.tsv\""
   ],
   "id": "b81494b637fbf846",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.224764Z",
     "start_time": "2025-02-02T16:07:53.222785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_file = \"/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/20241211_split.tsv\"\n",
    "config = {\n",
    "    'features': ['binned_peaks'],\n",
    "    'feature_attributes': {\n",
    "        'binned_peaks': {\n",
    "            'max_mz': 1000,\n",
    "            'bin_width': 0.25,\n",
    "            'to_rel_intensities': True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "featurizer = SpectrumFeaturizer(config, mode='torch')\n",
    "batch_size = 12\n",
    "input_dim = 4000"
   ],
   "id": "fee0ccc905710dd8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.545672Z",
     "start_time": "2025-02-02T16:07:53.230726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msn_dataset = MSnDataset(\n",
    "    pth=spectra_mgf,\n",
    "    featurizer=featurizer,\n",
    "    mol_transform=None,\n",
    "    max_allowed_deviation=0.005\n",
    ")"
   ],
   "id": "1bed961f9ccccb53",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.561020Z",
     "start_time": "2025-02-02T16:07:53.559041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module_msn = MassSpecDataModule(\n",
    "    dataset=msn_dataset,\n",
    "    batch_size=batch_size,\n",
    "    split_pth=split_file,\n",
    "    num_workers=0,\n",
    ")"
   ],
   "id": "e1cd1d99545d65ff",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.573799Z",
     "start_time": "2025-02-02T16:07:53.570651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SMILES_TOKENIZER_SAVE_PATH = \"/Users/macbook/CODE/Majer:MassSpecGym/data/tokenizers/smiles_tokenizer.json\"\n",
    "smiles_tokenizer = ByteBPETokenizerWithSpecialTokens(tokenizer_path=SMILES_TOKENIZER_SAVE_PATH)"
   ],
   "id": "419dfea34462c58f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from /Users/macbook/CODE/Majer:MassSpecGym/data/tokenizers/smiles_tokenizer.json.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.590531Z",
     "start_time": "2025-02-02T16:07:53.588757Z"
    }
   },
   "cell_type": "code",
   "source": "smiles_data = msn_dataset.smiles",
   "id": "8217b7f5c689813a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.619921Z",
     "start_time": "2025-02-02T16:07:53.617020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the dataset and dataloader\n",
    "pretrain_dataset = SMILESDataset(smiles_data, smiles_tokenizer, max_len=200)\n",
    "pretrain_dataloader = DataLoader(\n",
    "    pretrain_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: {\n",
    "        \"input\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [item[\"input\"] for item in batch], batch_first=False, padding_value=smiles_tokenizer.token_to_id(PAD_TOKEN)\n",
    "        ),\n",
    "        \"target\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [item[\"target\"] for item in batch], batch_first=False, padding_value=smiles_tokenizer.token_to_id(PAD_TOKEN)\n",
    "        )\n",
    "    }\n",
    ")"
   ],
   "id": "bccb8f6636ffbe2a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:07:53.731396Z",
     "start_time": "2025-02-02T16:07:53.625112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the language model\n",
    "vocab_size = smiles_tokenizer.get_vocab_size()\n",
    "model_pretrain = SMILESLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=1024,\n",
    "    nhead=4,\n",
    "    num_decoder_layers=4,\n",
    "    dropout=0.1,\n",
    "    pad_token_id=smiles_tokenizer.token_to_id(PAD_TOKEN),\n",
    "    max_len=200\n",
    ")\n"
   ],
   "id": "88c7d7abf48d7a93",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:18.100957Z",
     "start_time": "2025-02-02T16:07:53.743015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train using PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"cpu\", devices=1)  # adjust as available\n",
    "trainer.fit(model_pretrain, pretrain_dataloader)\n",
    "\n",
    "# Save the pretrained weights\n",
    "torch.save(model_pretrain.state_dict(), \"smiles_decoder_pretrained.pth\")\n",
    "print(\"Pretraining complete and weights saved.\")"
   ],
   "id": "bf837e18ad639717",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | decoder     | TransformerDecoder | 67.2 M | train\n",
      "1 | pos_encoder | PositionalEncoding | 0      | train\n",
      "2 | embedding   | Embedding          | 634 K  | train\n",
      "3 | fc_out      | Linear             | 635 K  | train\n",
      "4 | criterion   | CrossEntropyLoss   | 0      | train\n",
      "-----------------------------------------------------------\n",
      "68.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.5 M    Total params\n",
      "273.828   Total estimated model params size (MB)\n",
      "62        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a0882652d25423d93d65d92a04ef439"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining complete and weights saved.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:18.361628Z",
     "start_time": "2025-02-02T16:08:18.172381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_full = GATDeNovoTransformer(\n",
    "    input_dim=input_dim,  # example feature dimension from your spec.x shape\n",
    "    d_model=1024,\n",
    "    nhead=4,\n",
    "    num_gat_layers=3,\n",
    "    num_decoder_layers=4,\n",
    "    num_gat_heads=4,\n",
    "    gat_dropout=0.6,\n",
    "    smiles_tokenizer=smiles_tokenizer,\n",
    "    dropout=0.1,\n",
    "    max_smiles_len=200,\n",
    "    k_predictions=1,\n",
    "    temperature=1.0,\n",
    "    pre_norm=False,\n",
    "    chemical_formula=False\n",
    ")"
   ],
   "id": "7d45288ce4d74445",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:18.412547Z",
     "start_time": "2025-02-02T16:08:18.367996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pretrained decoder weights into the full model\n",
    "pretrained_dict = torch.load(\"smiles_decoder_pretrained.pth\", map_location=\"cpu\")\n",
    "model_dict = model_full.state_dict()"
   ],
   "id": "992a92904530432",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/73ckjztx5js4cc0cj85qgvlm0000gn/T/ipykernel_8145/568804105.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\"smiles_decoder_pretrained.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:18.488957Z",
     "start_time": "2025-02-02T16:08:18.484652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Only update the decoder-related parts (keys matching \"decoder\", \"pos_encoder\", \"decoder_embed\", \"decoder_fc\")\n",
    "pretrained_keys = {k: v for k, v in pretrained_dict.items() if k in model_dict and (\"decoder\" in k or \"pos_encoder\" in k or \"embedding\" in k or \"fc_out\" in k or \"decoder_embed\" in k or \"decoder_fc\" in k)}\n",
    "model_dict.update(pretrained_keys)\n",
    "model_full.load_state_dict(model_dict)\n",
    "print(\"Loaded pretrained decoder weights into the full model.\")"
   ],
   "id": "627871a19cb582ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained decoder weights into the full model.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:18.541612Z",
     "start_time": "2025-02-02T16:08:18.525714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=\"gat_de_novo_transformer\")\n",
    "freeze_callback = FreezeDecoderCallback(freeze_epochs=3)\n",
    "\n",
    "trainer_full = Trainer(\n",
    "    accelerator=\"cpu\",                # Change to \"gpu\" if available\n",
    "    devices=1,\n",
    "    max_epochs=2,                     # Adjust as needed\n",
    "    log_every_n_steps=10,\n",
    "    limit_train_batches=2,\n",
    "    limit_val_batches=2,\n",
    "    limit_test_batches=2,\n",
    "    logger=tb_logger,\n",
    "    callbacks=[freeze_callback],\n",
    ")"
   ],
   "id": "31a6464e8042593d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:08:37.030383Z",
     "start_time": "2025-02-02T16:08:18.614816Z"
    }
   },
   "cell_type": "code",
   "source": "trainer_full.fit(model_full, datamodule=data_module_msn)",
   "id": "8c1ae06d4116d418",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | gat_layers          | ModuleList         | 6.2 M  | train\n",
      "1 | encoder_fc          | Linear             | 1.0 M  | train\n",
      "2 | transformer_decoder | TransformerDecoder | 67.2 M | train\n",
      "3 | pos_encoder         | PositionalEncoding | 0      | train\n",
      "4 | decoder_embed       | Embedding          | 634 K  | train\n",
      "5 | decoder_fc          | Linear             | 635 K  | train\n",
      "6 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "7 | decoder_head_projs  | ModuleList         | 1.1 M  | train\n",
      "-------------------------------------------------------------------\n",
      "76.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "76.8 M    Total params\n",
      "307.047   Total estimated model params size (MB)\n",
      "78        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6dcc2753fcf47fbb336fbecddb2ec7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "137a6782d78a4af7b7ceb3f8d48fba19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Pretrained decoder frozen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab12c230be3b440e8cc8e183563448f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Pretrained decoder frozen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37917c2c558a4361a64ed037b2844499"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:09:31.029179Z",
     "start_time": "2025-02-02T16:08:37.102425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_full.test_beam_width = 3\n",
    "trainer_full.test(model_full, datamodule=data_module_msn)"
   ],
   "id": "311c2c3e6237e31b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b3a923d763f4596b64e814b8f8c0933"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:13] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "[17:09:13] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n",
      "N-]:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B�-]\n",
      "N-]' for input: 'SCCCC4\u000BCCCCSC\u000B�-]led parsing SMILES 'SCCCC4\u000BCCCCSC\u000B�-]\n",
      "[17:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:13] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "[17:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:13] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "[17:09:13] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n",
      "[17:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:13] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "[17:09:13] SMILES Parse Error: unclosed ring for input: 'SCCCC4�\u0019\u0005\u000B'\n",
      "N-]:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B�-]\n",
      "N-]' for input: 'SCCCC4\u000BCCCCSC\u000B�-]led parsing SMILES 'SCCCC4\u000BCCCCSC\u000B�-]\n",
      "[17:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:13] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "N-]:09:13] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B�-]\n",
      "N-]' for input: 'SCCCC4\u000BCCCCSC\u000B�-]led parsing SMILES 'SCCCC4\u000BCCCCSC\u000B�-]\n",
      "[17:09:13] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss               7.076807975769043\n",
      "    test_num_valid_mols                 0.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim            0.0\n",
      "   test_top_10_mces_dist               100.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim             0.0\n",
      "    test_top_1_mces_dist               100.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:31] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n",
      "[17:09:31] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n",
      "[17:09:31] SMILES Parse Error: unclosed ring for input: 'SCCCC4\u000B'\n",
      "[17:09:31] SMILES Parse Error: syntax error while parsing: SCCCC4\u000BCCCCSC\u000B\n",
      "[17:09:31] SMILES Parse Error: Failed parsing SMILES 'SCCCC4\u000BCCCCSC\u000B' for input: 'SCCCC4\u000BCCCCSC\u000B'\n",
      "[17:09:31] SMILES Parse Error: unclosed ring for input: 'SCCCC4�\u0019\u0005\u000B'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 7.076807975769043,\n",
       "  'test_num_valid_mols': 0.0,\n",
       "  'test_top_1_mces_dist': 100.0,\n",
       "  'test_top_1_max_tanimoto_sim': 0.0,\n",
       "  'test_top_1_accuracy': 0.0,\n",
       "  'test_top_10_mces_dist': 100.0,\n",
       "  'test_top_10_max_tanimoto_sim': 0.0,\n",
       "  'test_top_10_accuracy': 0.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5237b364953da2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc91ed5a70fac3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f923598458b98f01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T09:03:50.193176Z",
     "start_time": "2025-02-01T09:03:50.125701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import typing as T\n",
    "# from torch_geometric.nn import GATConv, global_mean_pool\n",
    "#\n",
    "# from massspecgym.models.base import Stage\n",
    "# from massspecgym.models.de_novo.base import DeNovoMassSpecGymModel\n",
    "#\n",
    "# # Adjust import paths if needed:\n",
    "# from phantoms.utils.custom_tokenizers import ByteBPETokenizerWithSpecialTokens\n",
    "# from phantoms.utils.constants import PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN\n",
    "#\n",
    "# from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class GATDeNovoTransformer(DeNovoMassSpecGymModel):\n",
    "    \"\"\"\n",
    "    Example GAT -> (single embedding) -> Transformer Decoder for SMILES generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,              # node feature dimension\n",
    "        d_model: int = 1024,\n",
    "        nhead: int = 4,\n",
    "        num_gat_layers: int = 3,\n",
    "        num_decoder_layers: int = 4,\n",
    "        num_gat_heads: int = 4,\n",
    "        gat_dropout: float = 0.6,\n",
    "        smiles_tokenizer: ByteBPETokenizerWithSpecialTokens = None,\n",
    "        start_token: str = SOS_TOKEN,\n",
    "        end_token: str = EOS_TOKEN,\n",
    "        pad_token: str = PAD_TOKEN,\n",
    "        unk_token: str = UNK_TOKEN,\n",
    "        dropout: float = 0.1,\n",
    "        max_smiles_len: int = 200,\n",
    "        k_predictions: int = 1,\n",
    "        temperature: T.Optional[float] = 1.0,\n",
    "        pre_norm: bool = False,\n",
    "        chemical_formula: bool = False,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # ------------------------------\n",
    "        #  1) SMILES Tokenizer\n",
    "        # ------------------------------\n",
    "        if smiles_tokenizer is None:\n",
    "            raise ValueError(\"Must provide a ByteBPETokenizerWithSpecialTokens instance.\")\n",
    "        self.smiles_tokenizer = smiles_tokenizer\n",
    "        self.vocab_size = self.smiles_tokenizer.get_vocab_size()\n",
    "\n",
    "        # Make sure special tokens exist in vocab\n",
    "        for tok in [start_token, end_token, pad_token, unk_token]:\n",
    "            if tok not in self.smiles_tokenizer.get_vocab():\n",
    "                raise ValueError(f\"Special token '{tok}' not in tokenizer vocab\")\n",
    "\n",
    "        # IDs\n",
    "        self.start_token_id = self.smiles_tokenizer.token_to_id(start_token)\n",
    "        self.end_token_id   = self.smiles_tokenizer.token_to_id(end_token)\n",
    "        self.pad_token_id   = self.smiles_tokenizer.token_to_id(pad_token)\n",
    "        self.unk_token_id   = self.smiles_tokenizer.token_to_id(unk_token)\n",
    "\n",
    "        # ------------------------------\n",
    "        #  2) Hyperparams\n",
    "        # ------------------------------\n",
    "        self.d_model = d_model\n",
    "        self.max_smiles_len = max_smiles_len\n",
    "        self.k_predictions = k_predictions\n",
    "        self.temperature = temperature if k_predictions > 1 else None\n",
    "        self.chemical_formula = chemical_formula\n",
    "\n",
    "        # ------------------------------\n",
    "        #  3) GAT Encoder\n",
    "        # ------------------------------\n",
    "        # We'll build a stack of GATConv => ELU\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "\n",
    "        # first layer\n",
    "        self.gat_layers.append(\n",
    "            GATConv(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=d_model // num_gat_heads,\n",
    "                heads=num_gat_heads,\n",
    "                dropout=gat_dropout,\n",
    "                add_self_loops=True\n",
    "            )\n",
    "        )\n",
    "        # subsequent layers\n",
    "        for _ in range(num_gat_layers - 1):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(\n",
    "                    in_channels=d_model,  # after heads, we combine into d_model\n",
    "                    out_channels=d_model // num_gat_heads,\n",
    "                    heads=num_gat_heads,\n",
    "                    dropout=gat_dropout,\n",
    "                    add_self_loops=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        #  4) Projection to d_model\n",
    "        # ------------------------------\n",
    "        self.encoder_fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # If you want formula => embed => add in:\n",
    "        if self.chemical_formula:\n",
    "            # Suppose formula is 128-dim or something\n",
    "            self.formula_mlp = nn.Sequential(\n",
    "                nn.Linear(128, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model, d_model),\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        #  5) Transformer Decoder\n",
    "        # ------------------------------\n",
    "        # We'll define a standard TransformerDecoder of `num_decoder_layers`\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"relu\",\n",
    "            batch_first=False,     # By default PyTorch uses seq_first\n",
    "            norm_first=pre_norm\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_smiles_len)\n",
    "\n",
    "        # Embeddings and final projection for SMILES tokens\n",
    "        self.decoder_embed = nn.Embedding(self.vocab_size, d_model, padding_idx=self.pad_token_id)\n",
    "        self.decoder_fc = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "        # We use a standard CE loss ignoring the pad index\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)\n",
    "\n",
    "        # init weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.encoder_fc.weight)\n",
    "        nn.init.zeros_(self.encoder_fc.bias)\n",
    "\n",
    "        nn.init.normal_(self.decoder_embed.weight, mean=0, std=self.d_model**-0.5)\n",
    "        nn.init.xavier_uniform_(self.decoder_fc.weight)\n",
    "        nn.init.zeros_(self.decoder_fc.bias)\n",
    "\n",
    "    # ------------------------------\n",
    "    #  TRAINING/VAL/TEST STEPS\n",
    "    # ------------------------------\n",
    "    def step(self, batch: dict, stage: Stage = Stage.NONE) -> dict:\n",
    "        \"\"\"\n",
    "        Mandatory method from base:\n",
    "          - forward pass,\n",
    "          - compute loss,\n",
    "          - optionally decode molecules.\n",
    "        \"\"\"\n",
    "        print(\"running\")\n",
    "        output_dict = self.forward(batch)\n",
    "        print(\"running\")\n",
    "        loss = output_dict[\"loss\"]\n",
    "\n",
    "        # Log the loss\n",
    "        self.log(\n",
    "            f\"{stage.to_pref()}loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=batch[\"spec\"].num_graphs,  # or batch['spec'].size(0) if you do that\n",
    "        )\n",
    "        print(\"running\")\n",
    "\n",
    "        # For steps where we want metrics, set `mols_pred` so we can evaluate\n",
    "        if stage not in self.log_only_loss_at_stages:\n",
    "            mols_pred = self.decode_smiles(batch)  # or do beam if k_predictions>1\n",
    "            output_dict[\"mols_pred\"] = mols_pred\n",
    "        else:\n",
    "            output_dict[\"mols_pred\"] = None\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def forward(self, batch: dict) -> dict:\n",
    "        \"\"\"\n",
    "        1) GAT-encode MSn trees => single [batch_size, d_model].\n",
    "        2) Teacher-forcing the SMILES decoder => compute CE loss.\n",
    "        \"\"\"\n",
    "        spec = batch[\"spec\"]  # PyG DataBatch: .x, .edge_index, .batch\n",
    "        smiles_list = batch[\"mol\"]  # list of SMILES strings\n",
    "\n",
    "        # ------------------------------\n",
    "        #  1) GAT ENCODER\n",
    "        # ------------------------------\n",
    "        x, edge_index, batch_idx = spec.x, spec.edge_index, spec.batch\n",
    "        # optional: debug shape\n",
    "        # print(\"x shape: \", x.shape, \"edge_index shape: \", edge_index.shape)\n",
    "\n",
    "        for layer_idx, gat in enumerate(self.gat_layers):\n",
    "            x = gat(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "\n",
    "        # global mean pool => [batch_size, d_model]\n",
    "        x = global_mean_pool(x, batch_idx)\n",
    "        print(\"running\")\n",
    "        # optional formula\n",
    "        if self.chemical_formula and (\"formula\" in batch):\n",
    "            # Suppose batch[\"formula\"] is shape [batch_size, 128]\n",
    "            formula = batch[\"formula\"].float().to(x.device)\n",
    "            x = x + self.formula_mlp(formula)\n",
    "\n",
    "        # project to exactly d_model (in case it changed shape)\n",
    "        memory = self.encoder_fc(x)  # shape [batch_size, d_model]\n",
    "\n",
    "        # We want shape [S=1, B, D] for the memory\n",
    "        memory = memory.unsqueeze(0)  # => [1, batch_size, d_model]\n",
    "\n",
    "        # ------------------------------\n",
    "        #  2) TOKENIZE SMILES + TEACHER-FORCE\n",
    "        # ------------------------------\n",
    "        encoded_smiles = self.smiles_tokenizer.encode_batch(smiles_list)  # list of lists\n",
    "        # Convert to a padded tensor\n",
    "        # If your ByteBPETokenizer is already applying padding to max_len, you get length = self.max_smiles_len\n",
    "        # shaped [batch_size, seq_len]\n",
    "        smiles_ids = torch.tensor(encoded_smiles, dtype=torch.long, device=x.device)\n",
    "\n",
    "        # We'll do teacher forcing:\n",
    "        #  input to the decoder: [  <s>  token1 token2 ... token_{n-1} ]\n",
    "        #  output to match:      [ token1 token2 ... token_{n-1}  </s> ]\n",
    "        # So let's shift everything by 1\n",
    "        tgt_input  = smiles_ids[:, :-1]  # [batch, seq_len-1]\n",
    "        tgt_output = smiles_ids[:, 1:]   # [batch, seq_len-1]\n",
    "        print(\"running\")\n",
    "        # Transpose to [seq_len-1, batch]\n",
    "        tgt_input  = tgt_input.transpose(0, 1).contiguous()   # => [tgt_len, batch]\n",
    "        tgt_output = tgt_output.transpose(0, 1).contiguous()  # => [tgt_len, batch]\n",
    "\n",
    "        # 2.1) Make a key_padding_mask for shape [batch, tgt_len]\n",
    "        #  True means \"ignore this position\"\n",
    "        tgt_key_padding_mask = (tgt_input == self.pad_token_id).transpose(0, 1)  # => [batch, tgt_len]\n",
    "\n",
    "        # 2.2) Embeddings\n",
    "        tgt_embed = self.decoder_embed(tgt_input) * math.sqrt(self.d_model)\n",
    "        # shape => [tgt_len, batch, d_model]\n",
    "\n",
    "        # 2.3) Positional encoding\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        print(\"running\")\n",
    "        # 2.4) Subsequent (causal) mask for the decoder\n",
    "        tgt_len = tgt_input.size(0)\n",
    "        causal_mask = self._generate_square_subsequent_mask(tgt_len).to(tgt_embed.device)\n",
    "\n",
    "        # 2.5) Pass through TransformerDecoder\n",
    "        # memory: [1, batch, d_model]\n",
    "        # tgt_embed: [tgt_len, batch, d_model]\n",
    "        decoded = self.transformer_decoder(\n",
    "            tgt=tgt_embed,\n",
    "            memory=memory,\n",
    "            tgt_mask=causal_mask,                 # shape [tgt_len, tgt_len]\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        # => [tgt_len, batch, d_model]\n",
    "\n",
    "        # 2.6) Final linear\n",
    "        logits = self.decoder_fc(decoded)  # => [tgt_len, batch, vocab_size]\n",
    "\n",
    "        # Flatten for CE loss\n",
    "        logits = logits.transpose(0, 1).contiguous()  # => [batch, tgt_len, vocab_size]\n",
    "        tgt_output = tgt_output.transpose(0, 1).contiguous()  # => [batch, tgt_len]\n",
    "\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.vocab_size),\n",
    "            tgt_output.view(-1)\n",
    "        )\n",
    "\n",
    "        return dict(loss=loss)\n",
    "\n",
    "    # ------------------------------\n",
    "    #  3) GREEDY/TEMPERATURE DECODING\n",
    "    # ------------------------------\n",
    "    def decode_smiles(self, batch: dict) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Generate up to k_predictions SMILES for each example in the batch, \n",
    "        returning a nested list of shape [batch_size, k_predictions].\n",
    "        We'll do a simple greedy or top-1 sampling approach here.\n",
    "        \"\"\"\n",
    "        spec = batch[\"spec\"]\n",
    "        x, edge_index, batch_idx = spec.x, spec.edge_index, spec.batch\n",
    "\n",
    "        # Encode GAT\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch_idx)\n",
    "\n",
    "        if self.chemical_formula and (\"formula\" in batch):\n",
    "            formula = batch[\"formula\"].float().to(x.device)\n",
    "            x = x + self.formula_mlp(formula)\n",
    "\n",
    "        memory = self.encoder_fc(x).unsqueeze(0)  # => [1, batch, d_model]\n",
    "\n",
    "        batch_size = memory.size(1)\n",
    "        device = memory.device\n",
    "\n",
    "        # We'll store k decoded sequences per example\n",
    "        all_decoded_smiles = [[] for _ in range(batch_size)]\n",
    "\n",
    "        # For simplicity, do k= self.k_predictions times\n",
    "        for _ in range(self.k_predictions):\n",
    "            # We'll do standard AR decoding up to max_smiles_len\n",
    "            # shape => [1, batch]\n",
    "            generated_tokens = torch.full(\n",
    "                (1, batch_size),\n",
    "                self.start_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "            finished = [False]*batch_size\n",
    "\n",
    "            decoded_sequences: T.List[T.List[int]] = [[] for _ in range(batch_size)]\n",
    "\n",
    "            for step in range(self.max_smiles_len):\n",
    "                tgt_embed = self.decoder_embed(generated_tokens) * math.sqrt(self.d_model)\n",
    "                tgt_embed = self.pos_encoder(tgt_embed)\n",
    "                tgt_len = tgt_embed.size(0)\n",
    "\n",
    "                causal_mask = self._generate_square_subsequent_mask(tgt_len).to(device)\n",
    "\n",
    "                # decode\n",
    "                output = self.transformer_decoder(\n",
    "                    tgt=tgt_embed,\n",
    "                    memory=memory,\n",
    "                    tgt_mask=causal_mask\n",
    "                )\n",
    "                # shape => [tgt_len, batch, d_model]\n",
    "\n",
    "                # final projection for last token\n",
    "                last_logits = self.decoder_fc(output[-1])  # => [batch, vocab_size]\n",
    "\n",
    "                if self.temperature is None:\n",
    "                    # Greedy\n",
    "                    next_token = torch.argmax(last_logits, dim=-1)  # => [batch]\n",
    "                else:\n",
    "                    # Temperature-based sampling\n",
    "                    probs = F.softmax(last_logits / self.temperature, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "                # append\n",
    "                next_token = next_token.unsqueeze(0)  # => [1, batch]\n",
    "                generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "\n",
    "                # check if ended\n",
    "                for i in range(batch_size):\n",
    "                    if not finished[i]:\n",
    "                        token_id = next_token[0, i].item()\n",
    "                        if token_id == self.end_token_id:\n",
    "                            finished[i] = True\n",
    "                        else:\n",
    "                            decoded_sequences[i].append(token_id)\n",
    "                if all(finished):\n",
    "                    break\n",
    "\n",
    "            # Now decode into text\n",
    "            # For each item in batch, convert token IDs -> string\n",
    "            batch_smiles = []\n",
    "            for seq_ids in decoded_sequences:\n",
    "                # We decode each token ID into text. \n",
    "                # NOTE: self.smiles_tokenizer.decode expects a *list of IDs for the entire sentence*.\n",
    "                # If your tokenizer merges multiple IDs into subwords, you might get partial fragments.\n",
    "                # But let's do it in one shot:\n",
    "                text = self.smiles_tokenizer.decode(seq_ids, skip_special_tokens=True)\n",
    "                batch_smiles.append(text)\n",
    "\n",
    "            # Store\n",
    "            for i in range(batch_size):\n",
    "                all_decoded_smiles[i].append(batch_smiles[i])\n",
    "\n",
    "        return all_decoded_smiles\n",
    "\n",
    "    # ------------------------------\n",
    "    #  4) Causal Mask Utility\n",
    "    # ------------------------------\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        2D causal mask for the target sequence of length sz.\n",
    "        shape => [sz, sz]\n",
    "          True => blocked,  False => allowed\n",
    "        PyTorch's Transformer can accept bool masks with True = no-attend.\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "        return mask"
   ],
   "id": "cd99e3563f0e039b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T09:03:51.188333Z",
     "start_time": "2025-02-01T09:03:51.183848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SMILES_TOKENIZER_SAVE_PATH = \"/Users/macbook/CODE/Majer:MassSpecGym/data/tokenizers/smiles_tokenizer.json\"\n",
    "smiles_tokenizer = ByteBPETokenizerWithSpecialTokens(tokenizer_path=SMILES_TOKENIZER_SAVE_PATH)"
   ],
   "id": "97388624181fe32e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from /Users/macbook/CODE/Majer:MassSpecGym/data/tokenizers/smiles_tokenizer.json.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:11:46.157725Z",
     "start_time": "2025-02-01T08:11:46.156051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spectra_mgf = \"/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/min_sample_trees.mgf\"\n",
    "split_file = \"/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/20241211_split.tsv\""
   ],
   "id": "71f03f723da5369e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:11:46.430608Z",
     "start_time": "2025-02-01T08:11:46.428540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'features': ['binned_peaks'],\n",
    "    'feature_attributes': {\n",
    "        'binned_peaks': {\n",
    "            'max_mz': 1000,\n",
    "            'bin_width': 0.25,\n",
    "            'to_rel_intensities': True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "featurizer = SpectrumFeaturizer(config, mode='torch')\n",
    "batch_size = 12"
   ],
   "id": "72409aaec2c71692",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:11:47.114141Z",
     "start_time": "2025-02-01T08:11:46.789659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msn_dataset = MSnDataset(\n",
    "    pth=spectra_mgf,\n",
    "    featurizer=featurizer,\n",
    "    mol_transform=None,\n",
    "    max_allowed_deviation=0.005\n",
    ")"
   ],
   "id": "d4d136f95480ef94",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:11:47.118973Z",
     "start_time": "2025-02-01T08:11:47.117099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module_msn = MassSpecDataModule(\n",
    "    dataset=msn_dataset,\n",
    "    batch_size=batch_size,\n",
    "    split_pth=split_file,\n",
    "    num_workers=0,\n",
    ")"
   ],
   "id": "b7aac4705e5789b0",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:11:47.282214Z",
     "start_time": "2025-02-01T08:11:47.212075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = 4000  # Update this based on your actual data\n",
    "\n",
    "# Initialize the Model\n",
    "model = GATDeNovoTransformer(\n",
    "    input_dim=input_dim,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=6,\n",
    "    num_gat_heads=8,\n",
    "    gat_dropout=0.6,\n",
    "    smiles_tokenizer=smiles_tokenizer,\n",
    "    start_token=SOS_TOKEN,\n",
    "    end_token=EOS_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    dropout=0.1,\n",
    "    max_smiles_len=200,\n",
    "    k_predictions=1,\n",
    "    temperature=1.0,\n",
    "    pre_norm=False,\n",
    "    chemical_formula=False  # Set to True if incorporating chemical formula embeddings\n",
    ")"
   ],
   "id": "1673f688b87b87fa",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T08:37:56.240164Z",
     "start_time": "2025-02-01T08:37:33.128831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Initialize TensorBoard Logger\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=\"gat_de_novo_transformer\")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",                # Change to \"gpu\" if available\n",
    "    devices=1,\n",
    "    max_epochs=20,                     # Adjust as needed\n",
    "    log_every_n_steps=10,\n",
    "    limit_train_batches=2,\n",
    "    limit_val_batches=2,\n",
    "    limit_test_batches=2,\n",
    "    logger=tb_logger,\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "trainer.fit(model, datamodule=data_module_msn)\n",
    "\n",
    "# Test the Model\n",
    "trainer.test(model, datamodule=data_module_msn)"
   ],
   "id": "b7e0a8785368bde2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                        | Type               | Params | Mode \n",
      "----------------------------------------------------------------------------\n",
      "0  | gat_layers                  | ModuleList         | 2.6 M  | train\n",
      "1  | encoder_fc                  | Linear             | 262 K  | train\n",
      "2  | transformer_decoder         | TransformerDecoder | 25.2 M | train\n",
      "3  | pos_encoder                 | PositionalEncoding | 0      | train\n",
      "4  | decoder_embed               | Embedding          | 317 K  | train\n",
      "5  | decoder_fc                  | Linear             | 318 K  | train\n",
      "6  | criterion                   | CrossEntropyLoss   | 0      | train\n",
      "7  | val_num_valid_mols          | MeanMetric         | 0      | train\n",
      "8  | val_top_1_mces_dist         | MeanMetric         | 0      | train\n",
      "9  | val_top_1_max_tanimoto_sim  | MeanMetric         | 0      | train\n",
      "10 | val_top_1_accuracy          | MeanMetric         | 0      | train\n",
      "11 | val_top_10_mces_dist        | MeanMetric         | 0      | train\n",
      "12 | val_top_10_max_tanimoto_sim | MeanMetric         | 0      | train\n",
      "13 | val_top_10_accuracy         | MeanMetric         | 0      | train\n",
      "----------------------------------------------------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.797   Total estimated model params size (MB)\n",
      "108       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "458e2defc3534c7ba9f1d67ddabf0654"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "running\n",
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    571\u001B[0m     ckpt_path,\n\u001B[1;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    574\u001B[0m )\n\u001B[0;32m--> 575\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 982\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1053\u001B[0m val_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[1;32m   1055\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:144\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:433\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    428\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    432\u001B[0m )\n\u001B[0;32m--> 433\u001B[0m output \u001B[38;5;241m=\u001B[39m call\u001B[38;5;241m.\u001B[39m_call_strategy_hook(trainer, hook_name, \u001B[38;5;241m*\u001B[39mstep_args)\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 323\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/massspecgym/models/base.py:66\u001B[0m, in \u001B[0;36mMassSpecGymModel.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28mself\u001B[39m, batch: \u001B[38;5;28mdict\u001B[39m, batch_idx: torch\u001B[38;5;241m.\u001B[39mTensor\n\u001B[1;32m     65\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(batch, stage\u001B[38;5;241m=\u001B[39mStage\u001B[38;5;241m.\u001B[39mVAL)\n",
      "Cell \u001B[0;32mIn[42], line 176\u001B[0m, in \u001B[0;36mGATDeNovoTransformer.step\u001B[0;34m(self, batch, stage)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_only_loss_at_stages:\n\u001B[0;32m--> 176\u001B[0m     mols_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_smiles(batch)  \u001B[38;5;66;03m# or do beam if k_predictions>1\u001B[39;00m\n\u001B[1;32m    177\u001B[0m     output_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmols_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m mols_pred\n",
      "Cell \u001B[0;32mIn[42], line 329\u001B[0m, in \u001B[0;36mGATDeNovoTransformer.decode_smiles\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# decode\u001B[39;00m\n\u001B[0;32m--> 329\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_decoder(\n\u001B[1;32m    330\u001B[0m     tgt\u001B[38;5;241m=\u001B[39mtgt_embed,\n\u001B[1;32m    331\u001B[0m     memory\u001B[38;5;241m=\u001B[39mmemory,\n\u001B[1;32m    332\u001B[0m     tgt_mask\u001B[38;5;241m=\u001B[39mcausal_mask\n\u001B[1;32m    333\u001B[0m )\n\u001B[1;32m    334\u001B[0m \u001B[38;5;66;03m# shape => [tgt_len, batch, d_model]\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;66;03m# final projection for last token\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:602\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 602\u001B[0m     output \u001B[38;5;241m=\u001B[39m mod(\n\u001B[1;32m    603\u001B[0m         output,\n\u001B[1;32m    604\u001B[0m         memory,\n\u001B[1;32m    605\u001B[0m         tgt_mask\u001B[38;5;241m=\u001B[39mtgt_mask,\n\u001B[1;32m    606\u001B[0m         memory_mask\u001B[38;5;241m=\u001B[39mmemory_mask,\n\u001B[1;32m    607\u001B[0m         tgt_key_padding_mask\u001B[38;5;241m=\u001B[39mtgt_key_padding_mask,\n\u001B[1;32m    608\u001B[0m         memory_key_padding_mask\u001B[38;5;241m=\u001B[39mmemory_key_padding_mask,\n\u001B[1;32m    609\u001B[0m         tgt_is_causal\u001B[38;5;241m=\u001B[39mtgt_is_causal,\n\u001B[1;32m    610\u001B[0m         memory_is_causal\u001B[38;5;241m=\u001B[39mmemory_is_causal,\n\u001B[1;32m    611\u001B[0m     )\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1095\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m   1089\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(\n\u001B[1;32m   1090\u001B[0m         x\n\u001B[1;32m   1091\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mha_block(\n\u001B[1;32m   1092\u001B[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001B[1;32m   1093\u001B[0m         )\n\u001B[1;32m   1094\u001B[0m     )\n\u001B[0;32m-> 1095\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1140\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._ff_block\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_ff_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1140\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear1(x))))\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout3(x)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 20\u001B[0m\n\u001B[1;32m      8\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      9\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m,                \u001B[38;5;66;03m# Change to \"gpu\" if available\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m     logger\u001B[38;5;241m=\u001B[39mtb_logger,\n\u001B[1;32m     17\u001B[0m )\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Train the Model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, datamodule\u001B[38;5;241m=\u001B[39mdata_module_msn)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Test the Model\u001B[39;00m\n\u001B[1;32m     23\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtest(model, datamodule\u001B[38;5;241m=\u001B[39mdata_module_msn)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 539\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[1;32m    540\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[1;32m    541\u001B[0m )\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33e1e0a035a469f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5aa80e40f791f584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7476d60a7042f1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a166ada68e44990e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d9d5ac90fe2f57d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "545578fa8fa14538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db7e1b7386a8d562"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9c5fccd33d60897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7eb6e2e74b4e6240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def is_valid_smiles(smiles: str) -> bool:\n",
    "#     try:\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         return mol is not None\n",
    "#     except:\n",
    "#         return False\n",
    "# \n",
    "# def calculate_uniqueness(smiles_list: T.List[str]) -> float:\n",
    "#     unique_smiles = set(smiles_list)\n",
    "#     return len(unique_smiles) / len(smiles_list)\n",
    "# \n",
    "# def calculate_novelty(smiles_list: T.List[str], training_set: T.Set[str]) -> float:\n",
    "#     novel_smiles = [smi for smi in smiles_list if smi not in training_set]\n",
    "#     return len(novel_smiles) / len(smiles_list)\n",
    "# \n",
    "# def calculate_diversity(smiles_list: T.List[str]) -> float:\n",
    "#     similarities = []\n",
    "#     for i in range(len(smiles_list)):\n",
    "#         mol1 = Chem.MolFromSmiles(smiles_list[i])\n",
    "#         if mol1 is None:\n",
    "#             continue\n",
    "#         fp1 = Chem.RDKFingerprint(mol1)\n",
    "#         for j in range(i + 1, len(smiles_list)):\n",
    "#             mol2 = Chem.MolFromSmiles(smiles_list[j])\n",
    "#             if mol2 is None:\n",
    "#                 continue\n",
    "#             fp2 = Chem.RDKFingerprint(mol2)\n",
    "#             similarity = TanimotoSimilarity(fp1, fp2)\n",
    "#             similarities.append(similarity)\n",
    "#     if similarities:\n",
    "#         average_similarity = sum(similarities) / len(similarities)\n",
    "#         diversity = 1 - average_similarity  # Higher diversity when lower similarity\n",
    "#     else:\n",
    "#         diversity = 0.0\n",
    "#     return diversity\n",
    "# \n",
    "# def validation_step(self, batch, batch_idx):\n",
    "#     outputs = self.forward(batch)\n",
    "#     loss = outputs['loss']\n",
    "#     self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "# \n",
    "#     if 'mols_pred' in outputs and outputs['mols_pred'] is not None:\n",
    "#         mols_pred = outputs['mols_pred']\n",
    "#         mols_true = batch['mol']\n",
    "# \n",
    "#         # Validity\n",
    "#         valid = [is_valid_smiles(smiles) for smiles in mols_pred]\n",
    "#         validity_score = sum(valid) / len(valid) if valid else 0\n",
    "#         self.log('val_validity', validity_score, on_epoch=True, prog_bar=True)\n",
    "# \n",
    "#         # Uniqueness\n",
    "#         uniqueness_score = calculate_uniqueness(mols_pred)\n",
    "#         self.log('val_uniqueness', uniqueness_score, on_epoch=True, prog_bar=True)\n",
    "# \n",
    "#         # Novelty (requires access to training set)\n",
    "#         # Assuming 'self.training_set' is defined elsewhere in the model\n",
    "#         if hasattr(self, 'training_set') and isinstance(self.training_set, set):\n",
    "#             novelty_score = calculate_novelty(mols_pred, self.training_set)\n",
    "#             self.log('val_novelty', novelty_score, on_epoch=True, prog_bar=True)\n",
    "# \n",
    "#         # Diversity\n",
    "#         diversity_score = calculate_diversity(mols_pred)\n",
    "#         self.log('val_diversity', diversity_score, on_epoch=True, prog_bar=True)\n",
    "# \n",
    "#     return {'val_loss': loss}"
   ],
   "id": "fba43b79b8a0435b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "63beeacff69a25f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "72bf19642f5f1a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c37c7fe2202ac5aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89ddd765c71da65f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "727b8cfc60d2f5d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87a4fa91f4acdc3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a883d881d4a8527d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "894513e83734608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73d2f69dbbf47918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b4f00d33dcc6da65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
