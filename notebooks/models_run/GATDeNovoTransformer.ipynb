{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import yaml\n",
    "from datetime import datetime\n",
    "import os\n",
    "from phantoms.models.denovo.parser import train_decoder\n",
    "from phantoms.utils.parser import train_model, extract_and_save_embeddings, validate_config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_decoder_local.yml\"\n",
    "# Load the config\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f) \n",
    "\n",
    "# Set up an experiment folder with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\",\n",
    "                                 f\"{timestamp}_{config['experiment_base_name']}\")\n"
   ],
   "id": "525a67e3074fb474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_decoder(config, experiment_folder, config_path)",
   "id": "65da377704ee2c2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train DeNovo",
   "id": "fd9e4c6abd8f67b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "validate_config(config)"
   ],
   "id": "59279255f9bdcffa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a unique experiment folder (for logs, checkpoints, configs, etc.)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "\n",
    "# In denovo training we do not need a cut_tree_level (or you may set it to None)\n",
    "cut_tree_level = None\n",
    "\n",
    "# Train the model (this will also initialize the data module, loggers, callbacks, etc.)\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "bf20c9b07556cf0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "ef7882a7d0cdffc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bonus De Novo",
   "id": "e7d7daeff67d232a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_bonus_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "validate_config(config)"
   ],
   "id": "97dbfe35ad356a93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a unique experiment folder (for logs, checkpoints, configs, etc.)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "\n",
    "# In denovo training we do not need a cut_tree_level (or you may set it to None)\n",
    "cut_tree_level = None\n",
    "\n",
    "# Train the model (this will also initialize the data module, loggers, callbacks, etc.)\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "d7895aedd7f8a27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "18be0cb98e7ec58d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run multi experiment",
   "id": "fa34ea29a9a674e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "from phantoms.utils.parser import validate_config, train_model, extract_and_save_embeddings\n",
    "from phantoms.optimizations.training import set_global_seeds\n",
    "\n",
    "def run_all_experiments(config_dir: str,\n",
    "                        experiment_parent_dir: str,\n",
    "                        config_files: List[str],\n",
    "                        cut_tree_levels: Optional[List[int]],\n",
    "                        wandb_project_name: str):\n",
    "    \"\"\"\n",
    "    Iterate over all configuration files and tree levels to run experiments.\n",
    "\n",
    "    Args:\n",
    "        config_dir (str): Directory containing configuration YAML files.\n",
    "        experiment_parent_dir (str): Parent directory to store all experiments.\n",
    "        config_files (List[str]): List of configuration YAML filenames.\n",
    "        cut_tree_levels (Optional[List[int]]): List of cut_tree_at_level values.\n",
    "        wandb_project_name (str): Name of the wandb project.\n",
    "    \"\"\"\n",
    "    # Set global seeds for reproducibility\n",
    "    set_global_seeds(42)\n",
    "\n",
    "    # Create the parent experiment directory if it doesn't exist\n",
    "    os.makedirs(experiment_parent_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each configuration file\n",
    "    for config_file in config_files:\n",
    "        config_path = os.path.join(config_dir, config_file)\n",
    "\n",
    "        if not os.path.exists(config_path):\n",
    "            print(f\"Configuration file {config_path} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load the configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        # Validate the configuration\n",
    "        try:\n",
    "            validate_config(config)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Configuration validation error in {config_file}: {ve}\")\n",
    "            continue\n",
    "\n",
    "        # Iterate over each tree level\n",
    "        for level in cut_tree_levels:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            # Adding a slight sleep to ensure different timestamps\n",
    "            time.sleep(1.0)\n",
    "\n",
    "            # Define a unique experiment folder name\n",
    "            config_name = os.path.splitext(os.path.basename(config_file))[0]\n",
    "            experiment_folder_name = f\"{config_name}_cut_tree_{level}_{timestamp}\"\n",
    "            experiment_folder = os.path.join(experiment_parent_dir, experiment_folder_name)\n",
    "\n",
    "            # Print experiment details for debugging\n",
    "            print(f\"\\nRunning Experiment: {experiment_folder_name}\")\n",
    "            print(f\"W&B Project: {wandb_project_name}\")\n",
    "            print(f\"Cut Tree Level: {level}\")\n",
    "\n",
    "            # Create the experiment folder\n",
    "            os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "            # Modify the config dict's 'trainer.checkpoint_dir' to point to experiment_folder subdirectories\n",
    "            config['trainer']['checkpoint_dir'] = os.path.join(experiment_folder, 'checkpoints')\n",
    "\n",
    "            # Optionally, modify 'experiment_base_name' to include the experiment_folder name or set to a unified value\n",
    "            config['experiment_base_name'] = 'experiment_trial'  # Or any desired base name\n",
    "\n",
    "            # Update W&B project name\n",
    "            config['wandb']['project'] = wandb_project_name\n",
    "\n",
    "            # Train the model\n",
    "            train_model(config, experiment_folder, config_path, level)\n",
    "\n",
    "            # Extract and save embeddings\n",
    "            extract_and_save_embeddings(config, level, experiment_folder)\n",
    "\n",
    "            # Finish the W&B run to ensure it's properly logged\n",
    "            wandb.finish()\n",
    "\n",
    "    print(\"\\nAll experiments completed successfully.\")"
   ],
   "id": "ffad437f137dcb22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "config_directory = '/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs'\n",
    "experiment_parent_directory = '/Users/macbook/CODE/PhantoMS/experiments_run/cut_trees_denovo'\n",
    "\n",
    "configuration_files = [\n",
    "    'config_denovo_local.yml',\n",
    "]\n",
    "tree_levels = [0, 1, 2, 3]\n",
    "\n",
    "wandb_project_name = 'cut_trees_denovo'\n",
    "\n",
    "# Authenticate with W&B\n",
    "print(\"Logging into Weights & Biases...\")\n",
    "wandb.login()\n",
    "\n",
    "# Run all experiments\n",
    "run_all_experiments(config_dir=config_directory,\n",
    "                    experiment_parent_dir=experiment_parent_directory,\n",
    "                    config_files=configuration_files,\n",
    "                    cut_tree_levels=tree_levels,\n",
    "                    wandb_project_name=wandb_project_name)"
   ],
   "id": "1f9b48425110e25a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UPDATED DE NOVO",
   "id": "c03d744572e3f41f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:34:27.812367Z",
     "start_time": "2025-02-21T17:34:23.706351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "from phantoms.utils.parser import validate_config, train_model, extract_and_save_embeddings"
   ],
   "id": "faee31607774ab53",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:34:27.820362Z",
     "start_time": "2025-02-21T17:34:27.815506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your local config file.\n",
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_spectra_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "validate_config(config)"
   ],
   "id": "caa1cf6429be7b3f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:08:47.118979Z",
     "start_time": "2025-02-21T18:08:47.115894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(f\"Experiment folder: {experiment_folder}\")"
   ],
   "id": "ea5efef0a00de0bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment folder: /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-08-47_denovo_bonus_test\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:08:50.287294Z",
     "start_time": "2025-02-21T18:08:47.471405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cut_tree_level = None\n",
    "\n",
    "# Train the final de novo model.\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "3ac9281202beebf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-08-47_denovo_bonus_test\n",
      "Using de novo task/model.\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/CODE/PhantoMS/phantoms/utils/parser.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_state = torch.load(config['model'].get('decoder_pretrained_path'), map_location=\"cpu\")\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained de_novo_scripts weights loaded into GATDeNovoTransformer.\n",
      "Loaded pretrained de_novo_scripts into de novo model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250221_190848-cv4s5133</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/cv4s5133' target=\"_blank\">2025-02-21_19-08-47_denovo_bonus_test</a></strong> to <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/cv4s5133' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/cv4s5133</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | gat_layers          | ModuleList         | 6.2 M  | train\n",
      "1 | formula_encoder     | Sequential         | 4.9 K  | train\n",
      "2 | encoder_fc          | Linear             | 1.1 M  | train\n",
      "3 | transformer_decoder | TransformerDecoder | 67.2 M | train\n",
      "4 | pos_encoder         | PositionalEncoding | 0      | train\n",
      "5 | decoder_embed       | Embedding          | 3.1 M  | train\n",
      "6 | decoder_fc          | Linear             | 3.1 M  | train\n",
      "7 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "80.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.7 M    Total params\n",
      "322.624   Total estimated model params size (MB)\n",
      "84        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27ea8b2e9ca24962aa27b9f87223f00f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    571\u001B[0m     ckpt_path,\n\u001B[1;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    574\u001B[0m )\n\u001B[0;32m--> 575\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 982\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1053\u001B[0m val_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[1;32m   1055\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:144\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:433\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    428\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    432\u001B[0m )\n\u001B[0;32m--> 433\u001B[0m output \u001B[38;5;241m=\u001B[39m call\u001B[38;5;241m.\u001B[39m_call_strategy_hook(trainer, hook_name, \u001B[38;5;241m*\u001B[39mstep_args)\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 323\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/massspecgym/models/base.py:66\u001B[0m, in \u001B[0;36mMassSpecGymModel.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28mself\u001B[39m, batch: \u001B[38;5;28mdict\u001B[39m, batch_idx: torch\u001B[38;5;241m.\u001B[39mTensor\n\u001B[1;32m     65\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(batch, stage\u001B[38;5;241m=\u001B[39mStage\u001B[38;5;241m.\u001B[39mVAL)\n",
      "File \u001B[0;32m~/CODE/PhantoMS/phantoms/models/denovo/GATDeNovoTransformer.py:148\u001B[0m, in \u001B[0;36mGATDeNovoTransformer.step\u001B[0;34m(self, batch, stage)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_only_loss_at_stages:\n\u001B[0;32m--> 148\u001B[0m     mols_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_smiles(batch, beam_width\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeam_width)\n\u001B[1;32m    149\u001B[0m     ret[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmols_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m mols_pred\n",
      "File \u001B[0;32m~/CODE/PhantoMS/phantoms/models/denovo/GATDeNovoTransformer.py:306\u001B[0m, in \u001B[0;36mGATDeNovoTransformer.decode_smiles\u001B[0;34m(self, batch, beam_width)\u001B[0m\n\u001B[1;32m    305\u001B[0m causal_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_square_subsequent_mask(tgt_len)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m--> 306\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_decoder(tgt\u001B[38;5;241m=\u001B[39mtgt_embed, memory\u001B[38;5;241m=\u001B[39mmemory_all\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), tgt_mask\u001B[38;5;241m=\u001B[39mcausal_mask)\n\u001B[1;32m    307\u001B[0m last_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder_fc(output[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:602\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 602\u001B[0m     output \u001B[38;5;241m=\u001B[39m mod(\n\u001B[1;32m    603\u001B[0m         output,\n\u001B[1;32m    604\u001B[0m         memory,\n\u001B[1;32m    605\u001B[0m         tgt_mask\u001B[38;5;241m=\u001B[39mtgt_mask,\n\u001B[1;32m    606\u001B[0m         memory_mask\u001B[38;5;241m=\u001B[39mmemory_mask,\n\u001B[1;32m    607\u001B[0m         tgt_key_padding_mask\u001B[38;5;241m=\u001B[39mtgt_key_padding_mask,\n\u001B[1;32m    608\u001B[0m         memory_key_padding_mask\u001B[38;5;241m=\u001B[39mmemory_key_padding_mask,\n\u001B[1;32m    609\u001B[0m         tgt_is_causal\u001B[38;5;241m=\u001B[39mtgt_is_causal,\n\u001B[1;32m    610\u001B[0m         memory_is_causal\u001B[38;5;241m=\u001B[39mmemory_is_causal,\n\u001B[1;32m    611\u001B[0m     )\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1087\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m   1085\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1086\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(\n\u001B[0;32m-> 1087\u001B[0m         x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001B[1;32m   1088\u001B[0m     )\n\u001B[1;32m   1089\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(\n\u001B[1;32m   1090\u001B[0m         x\n\u001B[1;32m   1091\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mha_block(\n\u001B[1;32m   1092\u001B[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001B[1;32m   1093\u001B[0m         )\n\u001B[1;32m   1094\u001B[0m     )\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1107\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._sa_block\u001B[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sa_block\u001B[39m(\n\u001B[1;32m   1101\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1102\u001B[0m     x: Tensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1105\u001B[0m     is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1106\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1107\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn(\n\u001B[1;32m   1108\u001B[0m         x,\n\u001B[1;32m   1109\u001B[0m         x,\n\u001B[1;32m   1110\u001B[0m         x,\n\u001B[1;32m   1111\u001B[0m         attn_mask\u001B[38;5;241m=\u001B[39mattn_mask,\n\u001B[1;32m   1112\u001B[0m         key_padding_mask\u001B[38;5;241m=\u001B[39mkey_padding_mask,\n\u001B[1;32m   1113\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[1;32m   1114\u001B[0m         need_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1115\u001B[0m     )[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m cut_tree_level \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Train the final de novo model.\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m train_model(config, experiment_folder, config_path, cut_tree_level)\n",
      "File \u001B[0;32m~/CODE/PhantoMS/phantoms/utils/parser.py:173\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(config, experiment_folder, config_file_path, cut_tree_level)\u001B[0m\n\u001B[1;32m    156\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(FreezeDecoderCallback(freeze_epochs\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfreeze_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m3\u001B[39m)))\n\u001B[1;32m    158\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[1;32m    159\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrainer\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerator\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    160\u001B[0m     devices\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrainer\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevices\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    170\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcallbacks\n\u001B[1;32m    171\u001B[0m )\n\u001B[0;32m--> 173\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, datamodule\u001B[38;5;241m=\u001B[39mdata_module)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeam_width\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    176\u001B[0m     orig_beam \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbeam_width\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 539\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[1;32m    540\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[1;32m    541\u001B[0m )\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:35:42.405681Z",
     "start_time": "2025-02-21T17:35:41.215543Z"
    }
   },
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "a9849676ea5bb460",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting embeddings for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-34-27_denovo_test\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n",
      "Embeddings saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-34-27_denovo_test/embeddings\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DeNovo Bonus",
   "id": "aaf5aba7392eba36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:59:48.893498Z",
     "start_time": "2025-02-21T17:59:48.888239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your local config file.\n",
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_spectra_bonus_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "validate_config(config)"
   ],
   "id": "c694058e404fff24",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:59:57.871506Z",
     "start_time": "2025-02-21T17:59:57.868920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(f\"Experiment folder: {experiment_folder}\")"
   ],
   "id": "2e832f5be2dcdf53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment folder: /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:04:39.933957Z",
     "start_time": "2025-02-21T18:00:05.209935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cut_tree_level = None\n",
    "\n",
    "# Train the final de novo model.\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "cc350ecc330518a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test\n",
      "Using de novo task/model.\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/CODE/PhantoMS/phantoms/utils/parser.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_state = torch.load(config['model'].get('decoder_pretrained_path'), map_location=\"cpu\")\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained de_novo_scripts weights loaded into GATDeNovoTransformer.\n",
      "Loaded pretrained de_novo_scripts into de novo model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250221_190005-n3sjwr29</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/n3sjwr29' target=\"_blank\">2025-02-21_18-59-57_denovo_bonus_test</a></strong> to <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/n3sjwr29' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/n3sjwr29</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | gat_layers          | ModuleList         | 6.2 M  | train\n",
      "1 | formula_encoder     | Sequential         | 4.9 K  | train\n",
      "2 | encoder_fc          | Linear             | 1.1 M  | train\n",
      "3 | transformer_decoder | TransformerDecoder | 67.2 M | train\n",
      "4 | pos_encoder         | PositionalEncoding | 0      | train\n",
      "5 | decoder_embed       | Embedding          | 3.1 M  | train\n",
      "6 | decoder_fc          | Linear             | 3.1 M  | train\n",
      "7 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "80.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.7 M    Total params\n",
      "322.624   Total estimated model params size (MB)\n",
      "84        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5ae71c11d604198b187d176e0e79e4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "[19:00:14] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCNCSCS CCCCSSCCCNCCCNC CCCCCCCCCCCCOSCOCBrCCCCN CCCCCCCCCCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCSCCBrCCNCCCCNCCCCN CNCCCOC CCCCCCCCCOCCCC CNCCCOC CCCCCCCCCOCCCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC�COCCOCCCCCCC CNCCCOC� CCCCCCCCCCCOCCCCCOCC CCCNCC CNCCCOC�COCCNCCNS CCCCCCCCCCCCOSSSCSSC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOCOCCCCCCSCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCC... truncated[19:00:14] SMILES Parse Error: Failed parsing SMILES ' CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCNCSCS CCCCSSCCCNCCCNC CCCCCCCCCCCCOSCOCBrCCCCN CCCCCCCCCCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCSCCBrCCNCCCCNCCCCN CNCCCOC CCCCCCCCCOCCCC CNCCCOC CCCCCCCCCOCCCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC�COCCOCCCCCCC CNCCCOC� CCCCCCCCCCCOCCCCCOCC CCCNCC CNCCCOC�COCCNCCNS CCCCCCCCCCCCOSSSCSSC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOCOCCCCCCSCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCC... truncated[19:00:14] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCNCSCS CCCCSSCCCNCCCNC CCCCCCCCCCCCOSCOCBrCCCCN CCCCCCCCCCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCSCCBrCCNCCCCNCCCCN CNCCCOC CCCCCCCCCOCCCC CNCCCOC CCCCCCCCCOCCCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC�COCCOCCCCCCC CNCCCOC� CCCCCCCCCCCOCCCCCOCCCCNCCCCNCCCCN CNCCCOC� CCCCCCCCCCCOCCCCCOCC CCCNCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOCOCCCCCCSCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCC... truncated[19:00:14] SMILES Parse Error: Failed parsing SMILES ' CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCNCSCS CCCCSSCCCNCCCNC CCCCCCCCCCCCOSCOCBrCCCCN CCCCCCCCCCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCSCCBrCCNCCCCNCCCCN CNCCCOC CCCCCCCCCOCCCC CNCCCOC CCCCCCCCCOCCCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC�COCCOCCCCCCC CNCCCOC� CCCCCCCCCCCOCCCCCOCCCCNCCCCNCCCCN CNCCCOC� CCCCCCCCCCCOCCCCCOCC CCCNCC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CNCCCOC CCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOCOCCCCCCSCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCC... truncated[19:00:22] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCSCS CCCCSSC CCCCCCCCCCCCOSCCCCCCCCCCCCCCCOCCCCNCCCNC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCNNCCNNCCNCNCC CNCCCOC CCCCCCCCCCCCOSCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBr CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCC... truncated[19:00:22] SMILES Parse Error: Failed parsing SMILES ' CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCSCS CCCCSSC CCCCCCCCCCCCOSCCCCCCCCCCCCCCCOCCCCNCCCNC CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCNNCCNNCCNCNCC CNCCCOC CCCCCCCCCCCCOSCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBr CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCC... truncated[19:00:22] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCN CCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCCCNNCNOC CCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBr)(= CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCC... truncated[19:00:22] SMILES Parse Error: Failed parsing SMILES ' CCCCCCCCCCCCOSCSCS CCCCCCCCCCCCOSCOCBr CCCCCCCCCCCCOSCOCBrCCCCN CCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCCCCCCCCCCCCCCCCCCCCCOCCCCCNNCNOC CCCCSSCCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBrCCNCCCNCCCOCCOCCOCCOCCOCCOCCOCCOCCCNNCCSCCBr)(= CCCCCCCCCCCCOS CCCCCCCCCOCCCCCNNCCNNCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCCCCCCCCCCCCCOCCCCCC... truncated/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61d7bbb2326c44caa47482c043c88b04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "993d0051990246f986b89e725908f535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:00:31] SMILES Parse Error: syntax error while parsing: C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(\n",
      "[19:00:31] SMILES Parse Error: Failed parsing SMILES 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(' for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C('\n",
      "[19:00:31] SMILES Parse Error: syntax error while parsing: C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(\n",
      "[19:00:31] SMILES Parse Error: Failed parsing SMILES 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(' for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C('\n",
      "[19:00:39] SMILES Parse Error: syntax error while parsing: C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(\n",
      "[19:00:39] SMILES Parse Error: Failed parsing SMILES 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(' for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C('\n",
      "[19:00:39] SMILES Parse Error: syntax error while parsing: C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(\n",
      "[19:00:39] SMILES Parse Error: Failed parsing SMILES 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(' for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C('\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbe2bb9956734994a3d0ee895afcbb64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13062d6c12064f18b07274277bbdda95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:45] SMILES Parse Error: syntax error while parsing: )C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:02:45] SMILES Parse Error: Failed parsing SMILES ')C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: ')C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:02:45] SMILES Parse Error: extra open parentheses for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(CO)C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:04:37] SMILES Parse Error: syntax error while parsing: )C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:04:37] SMILES Parse Error: Failed parsing SMILES ')C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: ')C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:04:37] SMILES Parse Error: extra open parentheses for input: 'C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(CC(C(C(C(C(C(C(C'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss               4.844666004180908\n",
      "    test_num_valid_mols                 0.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim            0.0\n",
      "   test_top_10_mces_dist               100.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim             0.0\n",
      "    test_top_1_mces_dist               100.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Model saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test/checkpoints/final_model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_num_valid_mols</td><td>▁</td></tr><tr><td>test_top_10_accuracy</td><td>▁</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_10_mces_dist</td><td>▁</td></tr><tr><td>test_top_1_accuracy</td><td>▁</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_1_mces_dist</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▆█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_num_valid_mols</td><td>▁█</td></tr><tr><td>val_top_10_accuracy</td><td>▁▁</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>▁█</td></tr><tr><td>val_top_10_mces_dist</td><td>▁▁</td></tr><tr><td>val_top_1_accuracy</td><td>▁▁</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>▁█</td></tr><tr><td>val_top_1_mces_dist</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>4.84467</td></tr><tr><td>test_num_valid_mols</td><td>0</td></tr><tr><td>test_top_10_accuracy</td><td>0</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_10_mces_dist</td><td>100</td></tr><tr><td>test_top_1_accuracy</td><td>0</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_1_mces_dist</td><td>100</td></tr><tr><td>trainer/global_step</td><td>4</td></tr><tr><td>val_loss</td><td>3.84818</td></tr><tr><td>val_num_valid_mols</td><td>1</td></tr><tr><td>val_top_10_accuracy</td><td>0</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>0.026</td></tr><tr><td>val_top_10_mces_dist</td><td>100</td></tr><tr><td>val_top_1_accuracy</td><td>0</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>0.026</td></tr><tr><td>val_top_1_mces_dist</td><td>100</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-02-21_18-59-57_denovo_bonus_test</strong> at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/n3sjwr29' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/n3sjwr29</a><br> View project at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250221_190005-n3sjwr29/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test/configs/config_denovo_spectra_bonus_local.yml\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:04:41.106283Z",
     "start_time": "2025-02-21T18:04:39.938817Z"
    }
   },
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "dabbb707907c7173",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting embeddings for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n",
      "Embeddings saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_18-59-57_denovo_bonus_test/embeddings\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train DreaMS",
   "id": "dac7caa120301e69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:32:36.720636Z",
     "start_time": "2025-02-21T18:32:36.714551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_dreams_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "validate_config(config)"
   ],
   "id": "b9452d3383522219",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:34:05.666574Z",
     "start_time": "2025-02-21T18:32:38.528464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a unique experiment folder (for logs, checkpoints, configs, etc.)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "\n",
    "# In denovo training we do not need a cut_tree_level (or you may set it to None)\n",
    "cut_tree_level = None\n",
    "\n",
    "# Train the model (this will also initialize the data module, loggers, callbacks, etc.)\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "383e244bbfd92d30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-32-38_denovo_dreams_test\n",
      "Using de novo task/model.\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/CODE/PhantoMS/phantoms/utils/parser.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_state = torch.load(config['model'].get('decoder_pretrained_path'), map_location=\"cpu\")\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | gat_layers          | ModuleList         | 3.2 M  | train\n",
      "1 | encoder_fc          | Linear             | 1.0 M  | train\n",
      "2 | transformer_decoder | TransformerDecoder | 67.2 M | train\n",
      "3 | pos_encoder         | PositionalEncoding | 0      | train\n",
      "4 | decoder_embed       | Embedding          | 3.1 M  | train\n",
      "5 | decoder_fc          | Linear             | 3.1 M  | train\n",
      "6 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "77.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "77.5 M    Total params\n",
      "310.153   Total estimated model params size (MB)\n",
      "79        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained de_novo_scripts weights loaded into GATDeNovoTransformer.\n",
      "Loaded pretrained de_novo_scripts into de novo model.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deffb628458345d99302802692b2cc58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "[19:33:03] non-ring atom 0 marked aromatic\n",
      "[19:33:03] non-ring atom 0 marked aromatic\n",
      "[19:33:12] non-ring atom 0 marked aromatic\n",
      "[19:33:12] Explicit valence for atom # 51 Cl, 2, is greater than permitted\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bec0683986648558403da15eda24c10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef0aec7d9bda4a8b9545162ab7115312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c81d49e3d0884b9c9dd6d022a2800f92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:33:39] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:39] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:33:39] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:39] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:33:46] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:46] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:33:46] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:46] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a54ae587a04a40f2840230b153bc21f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:33:55] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:55] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:33:55] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:33:55] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:34:03] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:34:03] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n",
      "[19:34:03] SMILES Parse Error: syntax error while parsing: ))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "[19:34:03] SMILES Parse Error: Failed parsing SMILES '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))' for input: '))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss               4.304252624511719\n",
      "    test_num_valid_mols                 0.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim            0.0\n",
      "   test_top_10_mces_dist               100.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim             0.0\n",
      "    test_top_1_mces_dist               100.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Model saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-32-38_denovo_dreams_test/checkpoints/final_model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_num_valid_mols</td><td>▁</td></tr><tr><td>test_top_10_accuracy</td><td>▁</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_10_mces_dist</td><td>▁</td></tr><tr><td>test_top_1_accuracy</td><td>▁</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_1_mces_dist</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▆█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_num_valid_mols</td><td>█▁</td></tr><tr><td>val_top_10_accuracy</td><td>▁▁</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>█▁</td></tr><tr><td>val_top_10_mces_dist</td><td>▁▁</td></tr><tr><td>val_top_1_accuracy</td><td>▁▁</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>█▁</td></tr><tr><td>val_top_1_mces_dist</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>4.30425</td></tr><tr><td>test_num_valid_mols</td><td>0</td></tr><tr><td>test_top_10_accuracy</td><td>0</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_10_mces_dist</td><td>100</td></tr><tr><td>test_top_1_accuracy</td><td>0</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_1_mces_dist</td><td>100</td></tr><tr><td>trainer/global_step</td><td>4</td></tr><tr><td>val_loss</td><td>3.79271</td></tr><tr><td>val_num_valid_mols</td><td>0</td></tr><tr><td>val_top_10_accuracy</td><td>0</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>0</td></tr><tr><td>val_top_10_mces_dist</td><td>100</td></tr><tr><td>val_top_1_accuracy</td><td>0</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>0</td></tr><tr><td>val_top_1_mces_dist</td><td>100</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-02-21_19-08-47_denovo_bonus_test</strong> at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/cv4s5133' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/cv4s5133</a><br> View project at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250221_190848-cv4s5133/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-32-38_denovo_dreams_test/configs/config_denovo_dreams_local.yml\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:34:22.074050Z",
     "start_time": "2025-02-21T18:34:05.686398Z"
    }
   },
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "c89e2d3849d33b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting embeddings for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-32-38_denovo_dreams_test\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n",
      "Embeddings saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-32-38_denovo_dreams_test/embeddings\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train DreaMS Bonus",
   "id": "3d8a3668fae099df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:35:26.458449Z",
     "start_time": "2025-02-21T18:35:26.452186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"/Users/macbook/CODE/PhantoMS/phantoms/models/denovo/configs/config_denovo_dreams_bonus_local.yml\"\n",
    "\n",
    "# Load configuration.\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "validate_config(config)"
   ],
   "id": "aee502da8f00164d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:40:06.509720Z",
     "start_time": "2025-02-21T18:35:26.814338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a unique experiment folder (for logs, checkpoints, configs, etc.)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_folder = os.path.join(\"/Users/macbook/CODE/PhantoMS/experiments_run\", f\"{timestamp}_{config['experiment_base_name']}\")\n",
    "\n",
    "# In denovo training we do not need a cut_tree_level (or you may set it to None)\n",
    "cut_tree_level = None\n",
    "\n",
    "# Train the model (this will also initialize the data module, loggers, callbacks, etc.)\n",
    "train_model(config, experiment_folder, config_path, cut_tree_level)"
   ],
   "id": "1df31f22109a1fe9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-35-26_denovo_dreams_bonus_test\n",
      "Using de novo task/model.\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/CODE/PhantoMS/phantoms/utils/parser.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_state = torch.load(config['model'].get('decoder_pretrained_path'), map_location=\"cpu\")\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | gat_layers          | ModuleList         | 3.2 M  | train\n",
      "1 | formula_encoder     | Sequential         | 4.9 K  | train\n",
      "2 | encoder_fc          | Linear             | 1.1 M  | train\n",
      "3 | transformer_decoder | TransformerDecoder | 67.2 M | train\n",
      "4 | pos_encoder         | PositionalEncoding | 0      | train\n",
      "5 | decoder_embed       | Embedding          | 3.1 M  | train\n",
      "6 | decoder_fc          | Linear             | 3.1 M  | train\n",
      "7 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "77.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "77.6 M    Total params\n",
      "310.435   Total estimated model params size (MB)\n",
      "84        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained de_novo_scripts weights loaded into GATDeNovoTransformer.\n",
      "Loaded pretrained de_novo_scripts into de novo model.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Train dataset size: 82\n",
      "Val dataset size: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21c8605bf4894200878bc1d3cfbddf96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6a023a312604e4a9693b6aa69cee9bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f877c6e63504205808f9a25b6d94e1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:36:05] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)C)CC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCC)C)CCCCCCCCCCCCCCCCCCCC\n",
      "[19:36:05] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)C)CC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCC)C)CCCCCCCCCCCCCCCCCCCC' for input: 'CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)C)CC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCC)C)CCCCCCCCCCCCCCCCCCCC'\n",
      "[19:36:05] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)CCCC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "[19:36:05] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)CCCC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC' for input: 'CCCCCCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)C)CCCCCCCCCCC)C)C)CCCC)C)C)C)CCCCCC)CCCC)C)C)C)C)CCCCCCCCCCCCCCC)C)CCCCC)CCCCC)C)C)C)C)CCCC)C)CCC)C)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC'\n",
      "[19:36:13] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)CCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "[19:36:13] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)CCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC' for input: 'CCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCC)C)C)C)C)C)C)CCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abdf075d51ce4900a03eaa96d647c33d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:36:21] SMILES Parse Error: syntax error while parsing: =C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:36:21] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:36:21] SMILES Parse Error: syntax error while parsing: =C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:36:21] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:36:28] SMILES Parse Error: syntax error while parsing: =C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(C(=C(=C(=C(=C(=C(C(C(=C(=C(=C(C(C(C(C(C(=C(=C(=C(=C(C(C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(=C(C(C(C(=C(C(=C(=C(=C(=C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C\n",
      "[19:36:28] SMILES Parse Error: Failed parsing SMILES '=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(C(=C(=C(=C(=C(=C(C(C(=C(=C(=C(C(C(C(C(C(=C(=C(=C(=C(C(C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(=C(C(C(C(=C(C(=C(=C(=C(=C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C' for input: '=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(C(=C(=C(=C(=C(=C(C(C(=C(=C(=C(C(C(C(C(C(=C(=C(=C(=C(C(C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(C(C(C(=C(C(C(C(=C(C(=C(=C(=C(=C(C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C(=C'\n",
      "[19:36:28] SMILES Parse Error: syntax error while parsing: =C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:36:28] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/phantoms_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f348a84834c148f5840cf6382a5099a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:38:12] SMILES Parse Error: syntax error while parsing: =C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:38:12] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:38:12] SMILES Parse Error: syntax error while parsing: =C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:38:12] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:40:03] SMILES Parse Error: syntax error while parsing: =C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:40:03] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n",
      "[19:40:03] SMILES Parse Error: syntax error while parsing: =C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C\n",
      "[19:40:03] SMILES Parse Error: Failed parsing SMILES '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C' for input: '=C(C(C(C=C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C(C'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         test_loss               4.326637268066406\n",
      "    test_num_valid_mols                 0.0\n",
      "    test_top_10_accuracy                0.0\n",
      "test_top_10_max_tanimoto_sim            0.0\n",
      "   test_top_10_mces_dist               100.0\n",
      "    test_top_1_accuracy                 0.0\n",
      "test_top_1_max_tanimoto_sim             0.0\n",
      "    test_top_1_mces_dist               100.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Model saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-35-26_denovo_dreams_bonus_test/checkpoints/final_model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_num_valid_mols</td><td>▁</td></tr><tr><td>test_top_10_accuracy</td><td>▁</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_10_mces_dist</td><td>▁</td></tr><tr><td>test_top_1_accuracy</td><td>▁</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>▁</td></tr><tr><td>test_top_1_mces_dist</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▆█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_num_valid_mols</td><td>█▁</td></tr><tr><td>val_top_10_accuracy</td><td>▁▁</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>█▁</td></tr><tr><td>val_top_10_mces_dist</td><td>▁▁</td></tr><tr><td>val_top_1_accuracy</td><td>▁▁</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>█▁</td></tr><tr><td>val_top_1_mces_dist</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_loss</td><td>4.32664</td></tr><tr><td>test_num_valid_mols</td><td>0</td></tr><tr><td>test_top_10_accuracy</td><td>0</td></tr><tr><td>test_top_10_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_10_mces_dist</td><td>100</td></tr><tr><td>test_top_1_accuracy</td><td>0</td></tr><tr><td>test_top_1_max_tanimoto_sim</td><td>0</td></tr><tr><td>test_top_1_mces_dist</td><td>100</td></tr><tr><td>trainer/global_step</td><td>4</td></tr><tr><td>val_loss</td><td>3.64926</td></tr><tr><td>val_num_valid_mols</td><td>0</td></tr><tr><td>val_top_10_accuracy</td><td>0</td></tr><tr><td>val_top_10_max_tanimoto_sim</td><td>0</td></tr><tr><td>val_top_10_mces_dist</td><td>100</td></tr><tr><td>val_top_1_accuracy</td><td>0</td></tr><tr><td>val_top_1_max_tanimoto_sim</td><td>0</td></tr><tr><td>val_top_1_mces_dist</td><td>100</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-02-21_19-34-22_denovo_dreams_bonus_test</strong> at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/0g9m8iix' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder/runs/0g9m8iix</a><br> View project at: <a href='https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder' target=\"_blank\">https://wandb.ai/jozefov-iocb-prague/PhantoMS_Decoder</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250221_193437-0g9m8iix/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-35-26_denovo_dreams_bonus_test/configs/config_denovo_dreams_bonus_local.yml\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:40:25.945713Z",
     "start_time": "2025-02-21T18:40:06.585630Z"
    }
   },
   "cell_type": "code",
   "source": "extract_and_save_embeddings(config, cut_tree_level, experiment_folder)",
   "id": "43e8274c45eb79f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting embeddings for /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-35-26_denovo_dreams_bonus_test\n",
      "Loaded tokenizer from /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json.\n",
      "Warning: Some identifiers in the split file are not found in the dataset. Taking intersection.\n",
      "Test dataset size: 17\n",
      "Embeddings saved to /Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_19-35-26_denovo_dreams_bonus_test/embeddings\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff5ce04ea96ee0ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
