#data:
#  molecule_tsv: "/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/min_sample_sample.tsv"
#  batch_size: 32
#
#model:
#  max_len: 200
#  d_model: 1024
#  nhead: 4
#  num_decoder_layers: 4
#  dropout_rate: 0.1
#  vocab_size: 1000
#  min_frequency: 2
#  # Path to the pretrained tokenizer JSON file
#  smiles_tokenizer_path: "/Users/macbook/CODE/PhantoMS/experiments_run/2025-02-03_17-46-47_tokenizer_test/smiles_tokenizer.json"
#
#trainer:
#  max_epochs: 2
#  accelerator: "cpu"
#  devices: 1
#  log_every_n_steps: 1
#
#experiment_base_name: "decoder_test"
#
#wandb:
#  project: "PhantoMS_Decoder"
#  entity: "jozefov-iocb-prague"

data:
  molecule_tsvs:
    - "/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/20241211_1M_murcko_train_smiles.tsv"
    - "/Users/macbook/CODE/Majer:MassSpecGym/data/MSn/20241211_4M_murcko_train_smiles.tsv"
  batch_size: 32

model:
  max_len: 200           # Maximum SMILES length for the decoder (not for tokenizer)
  vocab_size: 3000
  min_frequency: 2
  d_model: 1024
  nhead: 4
  num_decoder_layers: 4
  dropout_rate: 0.1
  # Local path to the pretrained tokenizer JSON file.
  smiles_tokenizer_path: "/Users/macbook/CODE/PhantoMS/experiments_run/2025-02-21_09-07-15_tokenizer_large/smiles_tokenizer.json"

trainer:
  max_epochs: 2          # For a quick test locally
  accelerator: "cpu"     # Change to "gpu" if you want to use GPU locally
  devices: 1
  log_every_n_steps: 10

experiment_base_name: "decoder_training"

wandb:
  project: "PhantoMS_Decoder"
  entity: "jozefov-iocb-prague"